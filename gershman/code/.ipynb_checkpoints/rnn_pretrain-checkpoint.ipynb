{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e92ac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88b2d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_RNN_TWO(nn.Module):\n",
    "    \n",
    "    \"\"\"     \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,input_size, hidden_size, num_parameters_embedding, output_size, dropout):\n",
    "        super(GRU_RNN_TWO, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.num_parameters_embedding = num_parameters_embedding\n",
    "        \n",
    "        \n",
    "        self.hidden_0 = nn.GRU(  \n",
    "                    input_size=input_size,\n",
    "                    hidden_size=hidden_size,\n",
    "                    num_layers=1,\n",
    "                    batch_first=True,\n",
    "                    dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.hidden_1 = nn.GRU(  \n",
    "                    input_size=input_size + num_parameters_embedding,\n",
    "                    hidden_size=hidden_size,\n",
    "                    num_layers=1,\n",
    "                    batch_first=True,\n",
    "                    dropout=dropout \n",
    "        )\n",
    "    \n",
    "        self.out_0 = nn.Linear(hidden_size, num_parameters_embedding)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.reg_a = nn.Linear(num_parameters_embedding, 1)\n",
    "        self.reg_b = nn.Linear(num_parameters_embedding, 1)\n",
    "        \n",
    "        self.out_1 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # parameters estimation\n",
    "        output_0, hn_0 = self.hidden_0(x)\n",
    "        output_0 = self.out_0(output_0)\n",
    "        output_0 = self.relu(output_0)\n",
    "        \n",
    "        output_a = self.reg_a(output_0)\n",
    "        output_b = self.reg_b(output_0)\n",
    "\n",
    "        # concat input \n",
    "        input_1 = torch.concat([x[0],output_0[0]],dim=1)\n",
    "        input_1 = input_1.reshape(1,x.shape[1],self.input_size + self.num_parameters_embedding)\n",
    "        \n",
    "        # action predication\n",
    "        output_1, hn_1 = self.hidden_1(input_1)\n",
    "        output_1 = self.out_1(output_1)\n",
    "        output_1 = F.softmax(output_1,dim=-1)\n",
    "\n",
    "        return output_0, output_1, hn_0, hn_1, output_a, output_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccca69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class behavior_dataset(Dataset):\n",
    "    \"\"\"         \n",
    "    \"\"\"\n",
    "    def __init__(self,dataframe):\n",
    "        \n",
    "        # action one hot transformation \n",
    "        action = np.array(dataframe['action'])\n",
    "        if np.all(action == action[0]):\n",
    "            action = np.append(action,(1-action[0]))\n",
    "            action = torch.tensor((action).reshape(len(dataframe) + 1),dtype=int)\n",
    "            action_onehot = nn.functional.one_hot(action, len(action.unique()))\n",
    "            # delete last one\n",
    "            action_onehot = action_onehot[:-1]\n",
    "        else:\n",
    "            action = torch.tensor((action).reshape(len(dataframe)),dtype=int)\n",
    "            action_onehot = nn.functional.one_hot(action, len(action.unique()))\n",
    "        \n",
    "        # reward\n",
    "        reward = torch.tensor((np.array(dataframe['reward'])).reshape(len(dataframe)),dtype=int)\n",
    "        \n",
    "        # concatinating reward and action\n",
    "        reward_action = torch.cat([reward[ :, np.newaxis], action_onehot],1)\n",
    "        \n",
    "        # adding dummy zeros to the beginning and ignoring the last one\n",
    "        reward_action_shift = nn.functional.pad(reward_action,[0,0,1,0])[:-1]\n",
    "        \n",
    "        n_blocks = int(len(dataframe)/10)\n",
    "        reward_action_shift.reshape(n_blocks,10,INPUT_SIZE)[:,0,:] = torch.zeros(size=(n_blocks,INPUT_SIZE))\n",
    "        \n",
    "        # parameters one hot transformation \n",
    "        parameters_embedding = torch.tensor((np.array(dataframe['parameters_embedding'])).reshape(len(dataframe)),dtype=int)\n",
    "        # parameters one hot transformation \n",
    "        parameters_embedding = parameters_embedding.type(dtype=torch.float32)\n",
    "        \n",
    "        p_a = torch.tensor((np.array(dataframe['alpha'])).reshape(len(dataframe)),dtype=torch.float32)\n",
    "        p_b = torch.tensor((np.array(dataframe['beta'])).reshape(len(dataframe)),dtype=torch.float32)\n",
    "        \n",
    "        # network input \n",
    "        x = reward_action_shift\n",
    "        \n",
    "        # network output \n",
    "        y = torch.cat([action_onehot, parameters_embedding[ :, np.newaxis],\n",
    "                       p_a[ :, np.newaxis], p_b[ :, np.newaxis],\n",
    "                      ],1)\n",
    "  \n",
    "        self.x = x.type(dtype=torch.float32)\n",
    "        self.y = y.type(dtype=torch.float32)\n",
    "        self.len = len(dataframe)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.len    \n",
    "    \n",
    "class merge_behavior_dataset(Dataset):\n",
    "    \"\"\" \n",
    "    Merge Dataset of each agent to one dataset\n",
    "    \n",
    "    Args:\n",
    "        dataset_list: list of Dataset of all agent \n",
    "        n_trials: num_of_trials each agent was simulated\n",
    "        \n",
    "    Returns: \n",
    "        torch Dataset:\n",
    "        x: [reward_(t-1) , action_(t-1)] all agents\n",
    "        y: [action_t, parameter embedding] all agents\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_list, n_trials):\n",
    "        X = []\n",
    "        Y = []\n",
    "        for agent in dataset_list:\n",
    "            for i in range(n_trials):\n",
    "                X.append(agent[i][0])\n",
    "                Y.append(agent[i][1])\n",
    "                \n",
    "        self.x = torch.stack(X).type(dtype=torch.float32)\n",
    "        self.y = torch.stack(Y).type(dtype=torch.float32)\n",
    "        self.len = len(X)   \n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.len    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e713e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the data  \n",
    "\n",
    "# read file \n",
    "\n",
    "# path = f'../../data/artificial_trainset_2000_fix.csv'\n",
    "# path = f'../../data/artificial_trainset_2000_rapid.csv'\n",
    "\n",
    "path = f'../../data/artificial_trainset_2000.csv'\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.reward[df.reward == 5] = 1\n",
    "\n",
    "\n",
    "# define constant \n",
    "num_of_action = df['action'].nunique()\n",
    "num_of_trials = df['trial_num'].nunique()\n",
    "num_of_agents = df['agent_index'].nunique()\n",
    "num_parameters_embedding = df['parameters_embedding'].nunique()\n",
    "\n",
    "# netowrk input and output dimension  \n",
    "Input_size = 1 + num_of_action\n",
    "Output_size = num_of_action + num_parameters_embedding\n",
    "\n",
    "# train val test split \n",
    "n_agent_train = int(0.8*num_of_agents)\n",
    "n_agent_val = int(0.2*num_of_agents)\n",
    "\n",
    "all_data = []\n",
    "for i in range(num_of_agents):\n",
    "    s = i*num_of_trials\n",
    "    e = (i+1)*num_of_trials\n",
    "    cur_df = df.iloc[s:e]\n",
    "    cur_df = cur_df.reset_index()\n",
    "    all_data.append([i,behavior_dataset(cur_df)])\n",
    "    \n",
    "random.shuffle(all_data)\n",
    "all_data = np.array(all_data)\n",
    "train_dataset = all_data[:n_agent_train,1]\n",
    "train_dataset = merge_behavior_dataset(train_dataset,num_of_trials)\n",
    "\n",
    "val_dataset = all_data[n_agent_train:,1]\n",
    "val_dataset = merge_behavior_dataset(val_dataset,num_of_trials)\n",
    "val_aaa = np.array([all_data[i,0] for i in range(n_agent_train,num_of_agents)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe81452a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('num_of_trials:',num_of_trials)\n",
    "print('num_of_agents:',num_of_agents)\n",
    "print('num_parameters_embedding:',num_parameters_embedding)\n",
    "print('train_size:', n_agent_train*num_of_trials)\n",
    "print('train_size:', train_dataset[:][0].shape)\n",
    "print('val_size:', n_agent_val*num_of_trials)\n",
    "print('val_size:', val_dataset[:][0].shape)\n",
    "print('val_agents',val_aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b5769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_loss(y_hat_action, y_hat_parameters, y_hat_a, y_hat_b, y_true):\n",
    "    \n",
    "    # slice true action and true parameters embedding \n",
    "    y_true_action = y_true[:,:2]\n",
    "    y_true_parameters = torch.flatten(y_true[:,2])\n",
    "    y_true_parameters = y_true_parameters.type(dtype=torch.LongTensor).to(device)\n",
    "    \n",
    "    y_true_a =  y_true[:,3]\n",
    "    y_true_b =  y_true[:,4]\n",
    "    \n",
    "    # define losses\n",
    "    criterion0 = nn.BCELoss()\n",
    "    criterion1 = nn.CrossEntropyLoss()\n",
    "    criterion2 = nn.MSELoss()\n",
    "    \n",
    "    loss_0 = criterion0(y_hat_action, y_true_action)\n",
    "    loss_1 = criterion1(y_hat_parameters, y_true_parameters)\n",
    "    \n",
    "    loss_a = criterion2(y_hat_a,y_true_a)\n",
    "    loss_b = criterion2(y_hat_b,y_true_b)\n",
    "\n",
    "    # combine losses\n",
    "    total_loss = 10*loss_0 + loss_1 + 300*loss_a + 10*loss_b \n",
    "    \n",
    "    return total_loss, 10*loss_0, loss_1, 300*loss_a, 10*loss_b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebbb110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_two(net, train_loader, val_loader ,epochs, number):\n",
    "    \n",
    "    n_step = 1 # \n",
    "    index = 0 # \n",
    "    \n",
    "    min_loss_t = 100\n",
    "    min_loss_v = 100\n",
    "    \n",
    "    # array to track loss \n",
    "    train_loss_array = np.zeros(shape=(int(epochs/n_step),2)) \n",
    "    val_loss_array = np.zeros(shape=(int(epochs/n_step),2))\n",
    "    \n",
    "    # move net to GPU\n",
    "    net.to(device)\n",
    "\n",
    "    # Use Adam optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001) \n",
    "    \n",
    "    # start timer\n",
    "    start_time = time.time()   \n",
    "    \n",
    "\n",
    "    # Loop over epochs \n",
    "    for i in range(epochs):\n",
    "        running_loss_0, running_loss_1 = [], []\n",
    "        running_loss_a, running_loss_b = [], []\n",
    "        \n",
    "        # Randomize train batch example \n",
    "        train_loader = random.sample(list(train_loader), len(train_loader))\n",
    "      \n",
    "        # Loop over training batches\n",
    "        for j, (X, y_true) in enumerate(train_loader):\n",
    "    \n",
    "            X, y_true = X.to(device), y_true.to(device) # move to GPU\n",
    "            X = X.reshape(1,X.shape[0],Input_size) # reshape to  1 x trials x input_size\n",
    "            \n",
    "            optimizer.zero_grad()  # zero the gradient buffers\n",
    "            \n",
    "            y_hat_parameters, y_hat_action, hn_0, hn_1,  y_hat_a, y_hat_b  = net(X) # forward pass\n",
    "            \n",
    "            y_hat_action = (y_hat_action.view(-1, num_of_action)) # Reshape to (SeqLen x Batch, OutputSize)\n",
    "            \n",
    "            y_hat_parameters = (y_hat_parameters.view(-1, num_parameters_embedding)) # Reshape to (SeqLen x Batch, OutputSize)\n",
    "            \n",
    "            y_hat_a = (y_hat_a.view(-1)) # Reshape to (SeqLen x Batch, OutputSize)\n",
    "            y_hat_b = (y_hat_b.view(-1)) # Reshape to (SeqLen x Batch, OutputSize)\n",
    "\n",
    "            loss, l_0, l_1, loss_a, loss_b = multi_loss(y_hat_action, y_hat_parameters,y_hat_a, y_hat_b ,y_true) # compute loss\n",
    "            \n",
    "            loss.backward() # backprop the loss\n",
    "            optimizer.step() # update the weights \n",
    "            \n",
    "            \n",
    "            running_loss_0.append(l_0.item())\n",
    "            running_loss_1.append(l_1.item())\n",
    "            \n",
    "            running_loss_a.append(loss_a.item())\n",
    "            running_loss_b.append(loss_b.item())\n",
    "              \n",
    "        \n",
    "        running_loss_0 = np.array(running_loss_0).mean()\n",
    "        running_loss_1 = np.array(running_loss_1).mean()\n",
    "        \n",
    "        running_loss_reg = (np.array(running_loss_a) + np.array(running_loss_b)).mean()\n",
    "        \n",
    "        lv0, lv1, lva, lvb = eval_net_two(net,val_loader)\n",
    "        \n",
    "        print('loss bce action',running_loss_0)\n",
    "        print('loss ce param',running_loss_1)\n",
    "        \n",
    "        print('loss mse a',(np.array(running_loss_a).mean()))\n",
    "        print('loss mse b',(np.array(running_loss_b).mean()))\n",
    "        \n",
    "        # Compute the running loss every 30 steps and save model\n",
    "        if i % n_step == n_step-1:\n",
    "            \n",
    "            train_loss_array[index] = running_loss_0, running_loss_1\n",
    "            tloss = running_loss_0 + running_loss_1 + running_loss_reg\n",
    "            \n",
    "            val_loss_array[index] = lv0, lv1\n",
    "            vloss = lv0 + lv1 + lva + lvb\n",
    "            \n",
    "            if tloss <= min_loss_t:\n",
    "                checkpoint = {'epoch':i+1,'model_state':net.state_dict(),'optim_state':optimizer.state_dict(),'loss':tloss}\n",
    "                torch.save(checkpoint,f'checkpoint_best_train_{number}_twolayers.pth')\n",
    "                min_loss_t = tloss\n",
    "                \n",
    "            if vloss <= min_loss_v:\n",
    "                checkpoint = {'epoch':i+1,'model_state':net.state_dict(),'optim_state':optimizer.state_dict(),'loss':vloss}\n",
    "                torch.save(checkpoint,f'checkpoint_best_val_{number}_twolayers.pth')\n",
    "                min_loss_v = vloss\n",
    "                \n",
    "            print('Step {}, Train Loss {:0.4f}, Val Loss {:0.4f}, Time {:0.1f}s'.format(i+1, tloss, vloss, time.time() - start_time))\n",
    "\n",
    "                \n",
    "            index += 1\n",
    "            net.train()\n",
    "\n",
    "            \n",
    "    return net, train_loss_array , val_loss_array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1b9c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_net_two(net,val_loader):\n",
    "    running_loss_0, running_loss_1 = [], []\n",
    "    running_loss_a, running_loss_b = [], []\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        for j, (X, y_true) in enumerate(val_loader):\n",
    "            \n",
    "            X, y_true = X.to(device), y_true.to(device) # move to GPU\n",
    "            X = X.reshape(1,X.shape[0],Input_size) # reshape to  1 x trials x input_size\n",
    "            y_hat_parameters, y_hat_action, hn_0, hn_1,  y_hat_a, y_hat_b  = net(X) # forward pass\n",
    "            \n",
    "            y_hat_action = (y_hat_action.view(-1, num_of_action)) # Reshape to (SeqLen x Batch, OutputSize)\n",
    "            y_hat_parameters = (y_hat_parameters.view(-1, num_parameters_embedding)) # Reshape to (SeqLen x Batch, OutputSize)            \n",
    "            y_hat_a = (y_hat_a.view(-1)) # Reshape to (SeqLen x Batch, OutputSize)\n",
    "            y_hat_b = (y_hat_b.view(-1)) # Reshape to (SeqLen x Batch, OutputSize)\n",
    "            \n",
    "            loss, l_0, l_1, loss_a, loss_b = multi_loss(y_hat_action, y_hat_parameters,y_hat_a, y_hat_b ,y_true)\n",
    "            running_loss_0.append(l_0.item())\n",
    "            running_loss_1.append(l_1.item())\n",
    "            running_loss_a.append(loss_a.item())\n",
    "            running_loss_b.append(loss_b.item())\n",
    "            \n",
    "    l_0 = np.array(running_loss_0).mean()\n",
    "    l_1 = np.array(running_loss_1).mean()\n",
    "    loss_a = np.array(running_loss_a).mean()\n",
    "    loss_b = np.array(running_loss_b).mean()\n",
    "    \n",
    "    return l_0, l_1, loss_a, loss_b\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcff554",
   "metadata": {},
   "outputs": [],
   "source": [
    "Output_size = 2\n",
    "number = 0\n",
    "aBatch = [1000]\n",
    "aHidden = [32]\n",
    "\n",
    "for bs in aBatch:\n",
    "    for hs in aHidden:\n",
    "        train_loader = DataLoader(train_dataset,shuffle=False,batch_size=bs)\n",
    "        val_loader = DataLoader(val_dataset,shuffle=False,batch_size=bs)\n",
    "        rnn = GRU_RNN_TWO(\n",
    "                      input_size=Input_size,\n",
    "                      hidden_size=hs,\n",
    "                      num_parameters_embedding=num_parameters_embedding, \n",
    "                      output_size=Output_size,\n",
    "                      dropout=0.2\n",
    "                     ) \n",
    "        rnn, loss_train, loss_val = train_model_two(rnn, train_loader, val_loader, 100, number)  \n",
    "        print('Done',bs*hs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
