{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd720801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import time\n",
    "import pickle \n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "INPUT_SIZE = 4\n",
    "OUTPUT_SIZE = 2\n",
    "DROPOUT = 0\n",
    "HIDDEN_SIZE = 10\n",
    "BATCH_SIZE = 1000\n",
    "EPOCH = 1_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d6c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    def __init__(self,input_size, hidden_size, output_size):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden = nn.GRU(  \n",
    "                            input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            dropout=DROPOUT\n",
    "        )\n",
    "    \n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output, hn = self.hidden(x)\n",
    "        output = self.out(output)\n",
    "        \n",
    "        # action prediction \n",
    "        output = F.softmax(output,dim=-1) \n",
    "        return output, hn\n",
    "\n",
    "\n",
    "class behavior_dataset(Dataset):\n",
    "    \"\"\" \n",
    "    \"\"\"\n",
    "    def __init__(self,dataframe):\n",
    "        \n",
    "        # action one hot transformation \n",
    "        action = np.array(dataframe['action'])\n",
    "        if np.all(action == action[0]):\n",
    "            action = np.append(action,(1-action[0]))\n",
    "            action = torch.tensor((action).reshape(len(dataframe) + 1),dtype=int)\n",
    "            action_onehot = nn.functional.one_hot(action, len(action.unique()))\n",
    "            # delete last one\n",
    "            action_onehot = action_onehot[:-1]\n",
    "        else:\n",
    "            action = torch.tensor((action).reshape(len(dataframe)),dtype=int)\n",
    "            action_onehot = nn.functional.one_hot(action, len(action.unique()))\n",
    "        \n",
    "        # reward\n",
    "        reward = torch.tensor((np.array(dataframe['reward'])).reshape(len(dataframe)),dtype=int)\n",
    "        \n",
    "        # concatinating reward and action\n",
    "        reward_action = torch.cat([reward[ :, np.newaxis], action_onehot],1)\n",
    "        \n",
    "        # block\n",
    "        block = torch.tensor((np.array(dataframe['start'])).reshape(len(dataframe)),dtype=int)\n",
    "        \n",
    "        # concatinating block with and reward_action\n",
    "        reward_action_block = torch.cat([block[ :, np.newaxis], reward_action],1)\n",
    "        \n",
    "        # adding dummy zeros to the beginning and ignoring the last one\n",
    "        reward_action_block_shift = nn.functional.pad(reward_action_block,[0,0,1,0])[:-1]\n",
    "        \n",
    "        # network input \n",
    "        x = reward_action_block_shift\n",
    "        \n",
    "        # network output \n",
    "        y = action_onehot\n",
    "  \n",
    "        self.x = x.type(dtype=torch.float32)\n",
    "        self.y = y.type(dtype=torch.float32)\n",
    "        self.len = len(dataframe)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx],self.y[idx]\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.len    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29faa274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, train_loader, test_loader, epochs):\n",
    "    \n",
    "    train_loss_array, test_loss_array = [], []\n",
    "    p_r2_array, norm_ll_array = [], []\n",
    "    net.to(device)  # move net to GPU\n",
    "    \n",
    "    # Use Adam optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001) \n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Loop over epochs \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        running_loss = 0\n",
    "        # Loop over training batches\n",
    "        for j, (X, y_true) in enumerate(train_loader):\n",
    "    \n",
    "            X, y_true = X.to(device), y_true.to(device) # move to GPU\n",
    "            X = X.reshape(1,X.shape[0],INPUT_SIZE) # reshape to  1 x trials x input_size\n",
    "            \n",
    "            optimizer.zero_grad()  # zero the gradient buffers\n",
    "            y_hat_action, hn = net(X) # forward pass\n",
    "            y_hat_action = (y_hat_action.view(-1, OUTPUT_SIZE)) # Reshape to (SeqLen x Batch, OutputSize)\n",
    "            loss = criterion(y_hat_action, y_true)  # compute loss\n",
    "            loss.backward() # backprop the loss\n",
    "            optimizer.step() # update the weights \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        train_loss_array.append(running_loss/len(train_loader))\n",
    "        test_running_loss, test_p_r2, norm_ll = eval_net(net,test_loader)\n",
    "        test_loss_array.append(test_running_loss)\n",
    "        p_r2_array.append(test_p_r2)\n",
    "        norm_ll_array.append(norm_ll)        \n",
    "        net.train()\n",
    "            \n",
    "    return net, train_loss_array, test_loss_array, p_r2_array, norm_ll_array\n",
    "\n",
    "def eval_net(net, test_loader):\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        running_loss = 0\n",
    "        for j, (X, y_true) in enumerate(test_loader):\n",
    "            \n",
    "            X, y_true = X.to(device), y_true.to(device) # move to GPU\n",
    "            X = X.reshape(1,X.shape[0],INPUT_SIZE) # reshape to  1 x trials x input_size\n",
    "            out, hn = net(X)\n",
    "            out = out.view(-1, OUTPUT_SIZE)\n",
    "            loss = criterion(out, y_true)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # pseudo-r2 1-(L/R) L=likelihood model R = likelihood chance model\n",
    "            y_true = torch.argmax(y_true,1)\n",
    "            ll = out.gather(1, y_true.view(-1,1))\n",
    "            ll = torch.sum(torch.log(ll))\n",
    "            p_r2 = 1 - ( ll / ( len(y_true)*np.log(0.5) ) ) \n",
    "            norm_ll = torch.exp(ll/len(y_true))\n",
    "\n",
    "    return ( running_loss/len(test_loader) ) , p_r2.item() , norm_ll.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dda33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/for_plos.csv')\n",
    "df = df.rename(columns={'key':'action'})\n",
    "\n",
    "df.loc[df[\"action\"] == \"R1\", \"action\"] = 0\n",
    "df.loc[df[\"action\"] == \"R2\", \"action\"] = 1\n",
    "\n",
    "df['action'] = df['action'].astype(int)\n",
    "df['reward'] = df['reward'].astype(int)\n",
    "df['block'] = df['block'] - 1\n",
    "df['start'] = (df.block.shift() != df.block).astype(int)\n",
    "\n",
    "# create unique list of names\n",
    "UniqueNames = df.ID.unique()\n",
    "dic = {}\n",
    "for i in range(101):\n",
    "    dic[UniqueNames[i]] = i\n",
    "for i in range(101):\n",
    "    df.loc[df.ID == UniqueNames[i], 'ID'] = dic[UniqueNames[i]]\n",
    "df = df.rename(columns={'ID':'subject'}).copy()\n",
    "\n",
    "label = []\n",
    "for i in range(101):\n",
    "    if 'B' in df[df.subject==i].diag.values[0]:\n",
    "        label.append(0)\n",
    "    elif 'D' in df[df.subject==i].diag.values[0]:\n",
    "        label.append(1)\n",
    "    else:\n",
    "        label.append(2)    \n",
    "z = []\n",
    "b = []\n",
    "d = []\n",
    "h = []\n",
    "idx = [] \n",
    "\n",
    "for i in range(101):\n",
    "    if label[i] == 0:\n",
    "        b.append(df[df.subject==i]) \n",
    "    elif label[i] == 1:\n",
    "        d.append(df[df.subject==i])\n",
    "    else:\n",
    "        h.append(df[df.subject==i])\n",
    "\n",
    "        \n",
    "for i in range(len(b)):\n",
    "    z.append(b[i])\n",
    "    idx.append(np.repeat(i,len(b[i])))\n",
    "    \n",
    "last_i = i+1\n",
    "    \n",
    "for i in range(len(d)):\n",
    "    z.append(d[i])\n",
    "    idx.append(np.repeat(last_i+i,len(d[i])))\n",
    "\n",
    "last_i = last_i+i+1\n",
    "\n",
    "for i in range(len(h)):\n",
    "    z.append(h[i])\n",
    "    idx.append(np.repeat(last_i+i,len(h[i])))\n",
    "\n",
    "df = pd.concat(z).reset_index().drop(columns='index')\n",
    "df['subject'] = np.concatenate(idx)\n",
    "\n",
    "bipolar_data = df[df.subject<33]\n",
    "depression_data = df[(df.subject>=33) & (df.subject<67)]\n",
    "healthy_data = df[(df.subject>=67)]\n",
    "\n",
    "train_res_gru, test_res_gru, test_p_r2_gru, normlized_ll = [],[],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a3f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bipolar\n",
    "for i in tqdm(range(33)):\n",
    "    \n",
    "    train = bipolar_data[(bipolar_data['subject']!=i)].reset_index() # train on all subj!=i\n",
    "    test = bipolar_data[(bipolar_data['subject']==i)].reset_index() # leave one out i for test\n",
    "\n",
    "    train_data = behavior_dataset(train)\n",
    "    test_data = behavior_dataset(test)\n",
    "\n",
    "    train_loader = DataLoader(train_data,shuffle=False,batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_data,shuffle=False,batch_size=len(test_data))\n",
    "\n",
    "    rnn = GRU(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE)\n",
    "    rnn, train_loss, test_loss, test_p_r2, norm_ll = train_model(rnn, train_loader, test_loader, EPOCH)\n",
    "\n",
    "    train_res_gru.append(train_loss)\n",
    "    test_res_gru.append(test_loss)\n",
    "    test_p_r2_gru.append(test_p_r2)\n",
    "    normlized_ll.append(norm_ll)\n",
    "    \n",
    "# Depression\n",
    "for i in tqdm(range(34)):\n",
    "    \n",
    "    i = i+33\n",
    "    \n",
    "    train = depression_data[(depression_data['subject']!=i)].reset_index() # train on all subj!=i\n",
    "    test = depression_data[(depression_data['subject']==i)].reset_index() # leave one out i for test\n",
    "\n",
    "    train_data = behavior_dataset(train)\n",
    "    test_data = behavior_dataset(test)\n",
    "\n",
    "    train_loader = DataLoader(train_data,shuffle=False,batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_data,shuffle=False,batch_size=len(test_data))\n",
    "\n",
    "    rnn = GRU(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE)\n",
    "    rnn, train_loss, test_loss, test_p_r2, norm_ll = train_model(rnn, train_loader, test_loader, EPOCH)\n",
    "\n",
    "    train_res_gru.append(train_loss)\n",
    "    test_res_gru.append(test_loss)\n",
    "    test_p_r2_gru.append(test_p_r2)\n",
    "    normlized_ll.append(norm_ll)\n",
    "    \n",
    "# Healthy \n",
    "for i in tqdm(range(34)):\n",
    "    \n",
    "    i = i+67\n",
    "    \n",
    "    train = healthy_data[(healthy_data['subject']!=i)].reset_index() # train on all subj!=i\n",
    "    test = healthy_data[(healthy_data['subject']==i)].reset_index() # leave one out i for test\n",
    "\n",
    "    train_data = behavior_dataset(train)\n",
    "    test_data = behavior_dataset(test)\n",
    "\n",
    "    train_loader = DataLoader(train_data,shuffle=False,batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_data,shuffle=False,batch_size=len(test_data))\n",
    "\n",
    "    rnn = GRU(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE)\n",
    "    rnn, train_loss, test_loss, test_p_r2, norm_ll = train_model(rnn, train_loader, test_loader, EPOCH)\n",
    "\n",
    "    train_res_gru.append(train_loss)\n",
    "    test_res_gru.append(test_loss)\n",
    "    test_p_r2_gru.append(test_p_r2)\n",
    "    normlized_ll.append(norm_ll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4958d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(train_res_gru).reshape(101,EPOCH).mean(axis=0),label='tr_gru')\n",
    "plt.plot(np.array(test_res_gru).reshape(101,EPOCH).mean(axis=0),label='te_gru')\n",
    "\n",
    "all_bce_gru = np.zeros(101)\n",
    "all_pr_2_gru = np.zeros(101)\n",
    "all_norm_ll_gru = np.zeros(101)\n",
    "\n",
    "for i in range(33):\n",
    "    tar = np.array(test_res_gru)[:33][i]\n",
    "    mask = np.array([False if j==i else True for j in range(33)])\n",
    "    other = np.array(test_res_gru)[:33][mask]\n",
    "    ind = np.argmin(other.mean(axis=0))\n",
    "    all_bce_gru[i] = tar[ind]\n",
    "    all_pr_2_gru[i] = np.array(test_p_r2_gru).reshape(101,EPOCH)[i][ind]\n",
    "    all_norm_ll_gru[i] = np.array(normlized_ll).reshape(101,EPOCH)[i][ind]\n",
    "    \n",
    "for i in range(34):\n",
    "    tar = np.array(test_res_gru)[33:67][i]\n",
    "    mask = np.array([False if j==i else True for j in range(34)])\n",
    "    other = np.array(test_res_gru)[33:67][mask]\n",
    "    ind = np.argmin(other.mean(axis=0))\n",
    "    all_bce_gru[33+i] = tar[ind]\n",
    "    all_pr_2_gru[33+i] = np.array(test_p_r2_gru).reshape(101,EPOCH)[33+i][ind]\n",
    "    all_norm_ll_gru[33+i] = np.array(normlized_ll).reshape(101,EPOCH)[33+i][ind]\n",
    "    \n",
    "for i in range(34):\n",
    "    tar = np.array(test_res_gru)[67:][i]\n",
    "    mask = np.array([False if j==i else True for j in range(34)])\n",
    "    other = np.array(test_res_gru)[67:][mask]\n",
    "    ind = np.argmin(other.mean(axis=0))\n",
    "    all_bce_gru[67+i] = tar[ind]\n",
    "    all_pr_2_gru[67+i] = np.array(test_p_r2_gru).reshape(101,EPOCH)[67+i][ind]\n",
    "    all_norm_ll_gru[67+i] = np.array(normlized_ll).reshape(101,EPOCH)[67+i][ind]\n",
    "\n",
    "\n",
    "plt.axhline(y=all_bce_gru.mean(), ls='--',color=sns.color_palette('tab10')[1],label='gru')\n",
    "plt.axhline(y=-np.log(.5),ls='--',color='r',label='random_baseline')\n",
    "plt.axhline(y=0.511,ls='--',color='k',label='qp')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b998978",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = np.concatenate([np.repeat(df.diag.unique()[0],33),\n",
    "        np.repeat(df.diag.unique()[1],34),\n",
    "        np.repeat(df.diag.unique()[2],34)])\n",
    "\n",
    "pd.DataFrame({\n",
    "            'subject':np.arange(101),\n",
    "            'diag':diag,\n",
    "            'bce':all_bce_gru,\n",
    "            'psr2':all_pr_2_gru,\n",
    "            'norm_ll':all_norm_ll_gru}).to_csv('../results/dezfouli_drnn.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
